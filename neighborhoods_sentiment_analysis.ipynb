{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udacity Write A Data Science Blog Post Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project is part of The [Udacity](https://eu.udacity.com/) Data Scientist Nanodegree Program which is composed by:\n",
    "- Term 1\n",
    "    - Supervised Learning\n",
    "    - Deep Learning\n",
    "    - Unsupervised Learning\n",
    "- Term 2\n",
    "    - Write A Data Science Blog Post\n",
    "    - Disaster Response Pipelines\n",
    "    - Recommendation Engines\n",
    "    \n",
    "The goal of this project is to put in practice the technical skills teached during the program but manly to focus on the ability to effectively communicate the results of the analysis.\n",
    "    \n",
    "The **CRISP-DM** Process (Cross Industry Process for Data Mining):\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Prepare Data\n",
    "4. Data Modeling\n",
    "5. Evaluate the Results\n",
    "6. Deploy   \n",
    "\n",
    "### Software and Libraries\n",
    "This project uses Python 3.7.2 the following libraries:\n",
    "- NumPy\n",
    "- Pandas\n",
    "- nltk\n",
    "- scikit-learn\n",
    "- Matplotlib\n",
    "- seaborn\n",
    "- TextBlob\n",
    "- WordCloud\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "Looking at the suggested datasets I was pretty stuck because of the too many options. Then because with some friends we were pondering the idea to transfer in Milan to be closer to our working places I have decided to use Airbnb data to do a sentiment analysis of its neighborhoods. \n",
    "\n",
    "Questions:\n",
    "- Witch are the 5 best scoring neighborhood?\n",
    "- Witch are the 5 worst scoring neighborhood?\n",
    "- How much is different the overview of the neighborhood given from the hosts from the one given by the guests?\n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "As already said the dataset is provided by [Airbnb](http://insideairbnb.com/get-the-data.html) and is basically composed by:\n",
    "- **listings.csv**:\tDetailed Listings data for Milan\n",
    "- **calendar.csv**:\tDetailed Calendar Data for listings in Milan\n",
    "- **reviews.csv**:\tDetailed Review Data for listings in Milan\n",
    "- **summary_listings.csv**:\tSummary information and metrics for listings in Milan (good for visualisations).\n",
    "- **summary_reviews.csv**: Summary Review data and Listing ID (to facilitate time based analytics and visualisations linked to a listing).\n",
    "- **neighbourhoods.csv**: Neighbourhood list for geo filter. Sourced from city or open source GIS files.\n",
    "- **neighbourhoods.geojson**: GeoJSON file of neighbourhoods of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from src.config import DATA_FOLDER\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "\n",
    "df_listings_data = pd.read_csv(DATA_FOLDER + \"listings.csv\")\n",
    "df_calendar_data = pd.read_csv(DATA_FOLDER + \"calendar.csv\")\n",
    "df_reviews_data = pd.read_csv(DATA_FOLDER + \"reviews.csv\")\n",
    "# df_summary_listings_data = pd.read_csv(DATA_FOLDER + 'summary_listings.csv')\n",
    "# df_summary_reviews_data = pd.read_csv(DATA_FOLDER + 'summary_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical variables:\")\n",
    "\n",
    "for name, values in df_listings_data.items():\n",
    "    if values.dtype == np.float64 or values.dtype == np.int64:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical variables values:\")\n",
    "\n",
    "for name, values in df_listings_data.items():\n",
    "    if values.dtype != np.float64 and values.dtype != np.int64:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in df_listings_data.items():\n",
    "    if values.dtype != np.float64 and values.dtype != np.int64:\n",
    "        print(\"{name}: {value}\\n\".format(name=name, value=values.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical variables:\")\n",
    "\n",
    "for name, values in df_calendar_data.items():\n",
    "    if values.dtype == np.float64 or values.dtype == np.int64:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical variables values:\")\n",
    "\n",
    "for name, values in df_calendar_data.items():\n",
    "    if values.dtype != np.float64 and values.dtype != np.int64:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in df_calendar_data.items():\n",
    "    if values.dtype != np.float64 and values.dtype != np.int64:\n",
    "        print(\"{name}: {value}\\n\".format(name=name, value=values.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical variables:\")\n",
    "\n",
    "for name, values in df_reviews_data.items():\n",
    "    if values.dtype == np.float64 or values.dtype == np.int64:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical variables values:\")\n",
    "\n",
    "for name, values in df_reviews_data.items():\n",
    "    if values.dtype != np.float64 and values.dtype != np.int64:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in df_reviews_data.items():\n",
    "    if values.dtype != np.float64 and values.dtype != np.int64:\n",
    "        print(\"{name}: {value}\\n\".format(name=name, value=values.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Data Modeling\n",
    "\n",
    "Now looking at the columns of the datasets we can figure out which of them can be usefull to answer our questions, of course for our goal the main focus is on the neighbourhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned = df_listings_data[\n",
    "    [\n",
    "        \"id\",\n",
    "        # , 'name'\n",
    "        # , 'summary'\n",
    "        # , 'space'\n",
    "        # , 'description'\n",
    "        \"neighborhood_overview\",\n",
    "        # , 'transit'\n",
    "        # , 'access'\n",
    "        # , 'interaction'\n",
    "        # , 'house_rules'\n",
    "        # , 'host_about'\n",
    "        # , 'host_neighbourhood'\n",
    "        # , 'neighbourhood'\n",
    "        \"neighbourhood_cleansed\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_listings_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_listings_data_cleaned['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'id' as key in the dataframe\n",
    "\n",
    "# df_listings_data_cleaned.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listings_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the missing values\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df_listings_data_cleaned.isnull(), cmap=\"Blues\", cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_listings_data[\"neighbourhood_cleansed\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 85 **neighbourhood_cleansed** unique entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neighbourhood = list(df_listings_data[\"neighbourhood_cleansed\"].unique())\n",
    "list_neighbourhood = [neighbourhood.lower() for neighbourhood in list_neighbourhood]\n",
    "\n",
    "print(list_neighbourhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching online for [Milan's neighbourhoods](http://www.museomilano.it/mediateca/media-pg-5/) and after some data cleaning we have this list of 130 neighbourhoods:\n",
    "- ticinese\n",
    "- magenta\n",
    "- porta vercellina\n",
    "- cordusio\n",
    "- carrobbio\n",
    "- cinquevie\n",
    "- sant’ambrogio\n",
    "- verziere\n",
    "- san babila\n",
    "- brolo-pantano\n",
    "- duomo\n",
    "- castello\n",
    "- sempione\n",
    "- brera\n",
    "- borgo degli ortolani - chinatown\n",
    "- porta nuova\n",
    "- centrale\n",
    "- centro direzionale\n",
    "- porta garibaldi\n",
    "- porta venezia\n",
    "- risorgimento\n",
    "- porta vittoria\n",
    "- porta romana\n",
    "- citta’ studi\n",
    "- acquabella\n",
    "- porta monforte\n",
    "- calvairate\n",
    "- lazio\n",
    "- tertulliano\n",
    "- porta vigentina\n",
    "- porta genova\n",
    "- porta lodovica\n",
    "- bullona\n",
    "- taliedo mecenate\n",
    "- morsenchio\n",
    "- gamboloita\n",
    "- castagnedo\n",
    "- vigentino\n",
    "- corvetto\n",
    "- nosedo\n",
    "- santa giulia\n",
    "- rogoredo\n",
    "- triulzo superiore\n",
    "- ponte lambro\n",
    "- forlanini\n",
    "- monluè\n",
    "- guastalla\n",
    "- ortica\n",
    "- cavriano\n",
    "- lambrate\n",
    "- loreto\n",
    "- abadesse\n",
    "- ponte seveso\n",
    "- isola\n",
    "- tortona\n",
    "- washington\n",
    "- solari\n",
    "- navigli\n",
    "- san pietro\n",
    "- la maddalena\n",
    "- pagano\n",
    "- fopponino\n",
    "- lotto\n",
    "- molinazzo\n",
    "- vaiano valle\n",
    "- selvanesco\n",
    "- moncucco\n",
    "- san cristoforo\n",
    "- lorenteggio giambellino\n",
    "- primaticcio \n",
    "- arzaga\n",
    "- forze armate\n",
    "- bisceglie\n",
    "- quarto cagnino\n",
    "- quinto romano\n",
    "- baggio\n",
    "- muggiano\n",
    "- trenno\n",
    "- figino\n",
    "- lampugnano\n",
    "- gallaratese\n",
    "- cascina merlata\n",
    "- certosa\n",
    "- qt8\n",
    "- san siro\n",
    "- portello\n",
    "- cagnola\n",
    "- musocco\n",
    "- roserio\n",
    "- vialba\n",
    "- ronchetto sul naviglio\n",
    "- barona\n",
    "- boffalora\n",
    "- chiesa rossa\n",
    "- conca fallata\n",
    "- cantalupa\n",
    "- gratosoglio\n",
    "- macconago\n",
    "- quintosole\n",
    "- morivione\n",
    "- chiaravalle\n",
    "- casoretto\n",
    "- greco\n",
    "- bicocca\n",
    "- prato centenario\n",
    "- gorla\n",
    "- precotto\n",
    "- villa san giovanni\n",
    "- adriano\n",
    "- crescenzago\n",
    "- rottole\n",
    "- turro\n",
    "- maggiolina\n",
    "- montalbino\n",
    "- niguarda\n",
    "- tre torri\n",
    "- dergano\n",
    "- affori \n",
    "- bovisasca\n",
    "- comasina\n",
    "- bruzzano\n",
    "- bovisa \n",
    "- villa pizzone\n",
    "- quarto oggiaro\n",
    "- farini \n",
    "- la fontana\n",
    "- ronchetto delle rane\n",
    "- conchetta\n",
    "- porta volta\n",
    "- ghisolfa\n",
    "\n",
    "![title](img/quartieri_milano.jpg)\n",
    "\n",
    "As we can see not all the neighbourhoods are rappresented in the dataset and moreover there is not an exact mapping between the dataset and the real neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_real_neighbourhood = []\n",
    "\n",
    "with open(DATA_FOLDER + \"quartieri.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        item = line.replace(\"\\n\", \"\")  # remove linebreak\n",
    "        list_real_neighbourhood.append(item)\n",
    "\n",
    "print(len(list_real_neighbourhood))\n",
    "\n",
    "# print(list_real_neighbourhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_present(item, lista):\n",
    "    for elemento in lista:\n",
    "        if elemento in item or item in elemento:\n",
    "            return elemento\n",
    "    return False\n",
    "\n",
    "\n",
    "lista = [\"pippo\", \"pluto\", \"paperino\"]\n",
    "print(is_present(\"paperino\", lista))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mapping_neighbourhood_real_neighbourhood = []\n",
    "list_no_matched_data_neighbourhood_by_real_neighbourhood = []\n",
    "list_no_matched_real_neighbourhood_by_data_neighbourhood = []\n",
    "\n",
    "for neighbourhood in list_neighbourhood:\n",
    "    real_neighbourhood = is_present(neighbourhood, list_real_neighbourhood)\n",
    "    if real_neighbourhood is False:\n",
    "        list_no_matched_data_neighbourhood_by_real_neighbourhood.append(neighbourhood)\n",
    "    else:\n",
    "        list_mapping_neighbourhood_real_neighbourhood.append(\n",
    "            (neighbourhood, real_neighbourhood)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mapping_neighbourhood_real_neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the association made by our funciton we can see some errors that we must correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update wrong association ('ronchetto sul naviglio', 'navigli') and ('bovisa', 'bovisasca'),\n",
    "\n",
    "list_mapping_neighbourhood_real_neighbourhood_correct = []\n",
    "\n",
    "for i in range(len(list_mapping_neighbourhood_real_neighbourhood)):\n",
    "    if list_mapping_neighbourhood_real_neighbourhood[i][0] == \"ronchetto sul naviglio\":\n",
    "        list_mapping_neighbourhood_real_neighbourhood_correct.append(\n",
    "            (\"ronchetto sul naviglio\", \"ronchetto sul naviglio\")\n",
    "        )\n",
    "    elif list_mapping_neighbourhood_real_neighbourhood[i][0] == \"bovisa\":\n",
    "        list_mapping_neighbourhood_real_neighbourhood_correct.append(\n",
    "            (\"bovisa\", \"bovisa\")\n",
    "        )\n",
    "    else:\n",
    "        list_mapping_neighbourhood_real_neighbourhood_correct.append(\n",
    "            list_mapping_neighbourhood_real_neighbourhood[i]\n",
    "        )\n",
    "\n",
    "list_mapping_neighbourhood_real_neighbourhood = (\n",
    "    list_mapping_neighbourhood_real_neighbourhood_correct\n",
    ")\n",
    "list_mapping_neighbourhood_real_neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neighbourhood in list_real_neighbourhood:\n",
    "    if neighbourhood not in [\n",
    "        element[1] for element in list_mapping_neighbourhood_real_neighbourhood\n",
    "    ]:\n",
    "        list_no_matched_real_neighbourhood_by_data_neighbourhood.append(neighbourhood)\n",
    "\n",
    "print(len(list_mapping_neighbourhood_real_neighbourhood))\n",
    "print(len(list_no_matched_data_neighbourhood_by_real_neighbourhood))\n",
    "print(len(list_no_matched_real_neighbourhood_by_data_neighbourhood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_no_matched_data_neighbourhood_by_real_neighbourhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_no_matched_real_neighbourhood_by_data_neighbourhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do by hand the mapping of this no matched neighbourhood with the help of Google Maps:\n",
    "\n",
    "|     Data               |     Real                         |\n",
    "|------------------------|----------------------------------| \n",
    "| bande nere             | primaticcio                      |\n",
    "| buenos aires - venezia | porta venezia                    |\n",
    "| corsica                | acquabella                       |\n",
    "| de angeli - monte rosa | tre torri                        |\n",
    "| garibaldi repubblica   | porta garibaldi                  |\n",
    "| ortomercato            | calvairate                       |\n",
    "| padova                 | isola                            |\n",
    "| parco bosco in città   | quinto romano                    |\n",
    "| parco delle abbazie    | vaiano valle                     |\n",
    "| parco lambro - cimiano | lambrate                         |\n",
    "| parco nord             | bicocca                          |\n",
    "| qt 8                   | qt8                              |\n",
    "| ripamonti              | vigentino                        |\n",
    "| s. cristoforo          | san cristoforo                   |\n",
    "| s. siro                | san siro                         |\n",
    "| sacco                  | vialba                           |\n",
    "| sarpi                  | borgo degli ortolani - chinatown |\n",
    "| scalo romana           | vigentino                        |\n",
    "| selinunte              | san siro                         |\n",
    "| stadera                | chiesa rossa                     |\n",
    "| tibaldi                | conchetta                        |\n",
    "| umbria - molise        | calvairate                       |\n",
    "| viale monza            | gorla                            |\n",
    "| villapizzone           | villa pizzone                    |\n",
    "| xxii marzo             | porta vittoria                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_manual_mapping_neighbourhood_real_neighbourhood = [\n",
    "    (\"bande nere\", \"primaticcio\"),\n",
    "    (\"buenos aires - venezia\", \"porta venezia\"),\n",
    "    (\"corsica\", \"acquabella\"),\n",
    "    (\"de angeli - monte rosa\", \"tre torri\"),\n",
    "    (\"garibaldi repubblica\", \"porta garibaldi\"),\n",
    "    (\"ortomercato\", \"calvairate\"),\n",
    "    (\"padova\", \"isola\"),\n",
    "    (\"parco bosco in citt\\x85\", \"quinto romano\"),\n",
    "    (\"parco delle abbazie\", \"vaiano valle\"),\n",
    "    (\"parco lambro - cimiano\", \"lambrate\"),\n",
    "    (\"parco nord\", \"bicocca\"),\n",
    "    (\"qt 8\", \"qt8\"),\n",
    "    (\"ripamonti\", \"vigentino\"),\n",
    "    (\"s. cristoforo\", \"san cristoforo\"),\n",
    "    (\"s. siro\", \"san siro\"),\n",
    "    (\"sacco\", \"vialba\"),\n",
    "    (\"sarpi\", \"borgo degli ortolani - chinatown\"),\n",
    "    (\"scalo romana\", \"vigentin\"),\n",
    "    (\"selinunte\", \"san siro\"),\n",
    "    (\"stadera\", \"chiesa rossa\"),\n",
    "    (\"tibaldi\", \"conchetta\"),\n",
    "    (\"umbria - molise\", \"calvairate\"),\n",
    "    (\"viale monza\", \"gorla\"),\n",
    "    (\"villapizzone\", \"villa pizzone\"),\n",
    "    (\"xxii marzo\", \"porta vittoria\"),\n",
    "]\n",
    "\n",
    "for tupla in list_manual_mapping_neighbourhood_real_neighbourhood:\n",
    "    list_mapping_neighbourhood_real_neighbourhood.append(tupla)\n",
    "    list_no_matched_data_neighbourhood_by_real_neighbourhood.remove(tupla[0])\n",
    "    if tupla[1] in list_no_matched_real_neighbourhood_by_data_neighbourhood:\n",
    "        list_no_matched_real_neighbourhood_by_data_neighbourhood.remove(tupla[1])\n",
    "\n",
    "print(len(list_mapping_neighbourhood_real_neighbourhood))\n",
    "print(len(list_no_matched_data_neighbourhood_by_real_neighbourhood))\n",
    "print(len(list_no_matched_real_neighbourhood_by_data_neighbourhood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mapping_neighbourhood_real_neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not rappresented neighbourhoods\n",
    "\n",
    "print(list_no_matched_real_neighbourhood_by_data_neighbourhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's map in the dataframe **neighbourhood_cleansed** to the real neighbourhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_neighbourhood(data_neighbourhood, list_mapping):\n",
    "    for tupla in list_mapping:\n",
    "        if tupla[0] == data_neighbourhood:\n",
    "            return tupla[1]\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\n",
    "    get_real_neighbourhood(\n",
    "        \"villapizzone\", list_mapping_neighbourhood_real_neighbourhood\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map neighbourhood to real neighbourhood\n",
    "\n",
    "df_listings_data_cleaned[\"real_neighbourhood\"] = [\n",
    "    get_real_neighbourhood(\n",
    "        neighbourhood.lower(), list_mapping_neighbourhood_real_neighbourhood\n",
    "    )\n",
    "    for neighbourhood in df_listings_data_cleaned[\"neighbourhood_cleansed\"]\n",
    "]\n",
    "\n",
    "# Drop 'neighbourhood_cleansed' column\n",
    "\n",
    "df_listings_data_cleaned = df_listings_data_cleaned.drop(\n",
    "    columns=[\"neighbourhood_cleansed\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_real_neighbourhood = list(df_listings_data_cleaned[\"real_neighbourhood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_data_real_neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_data_real_neighbourhood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an histogram of the number of listings related to a neighborhood\n",
    "\n",
    "list_data_real_neighbourhood_count = []\n",
    "\n",
    "for data_real_neighbourhood in set(list_data_real_neighbourhood):\n",
    "    list_data_real_neighbourhood_count.append(\n",
    "        (\n",
    "            str(data_real_neighbourhood),\n",
    "            list_data_real_neighbourhood.count(str(data_real_neighbourhood)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# print(list_data_real_neighbourhood_count)\n",
    "\n",
    "data_real_neighbourhoods = [\n",
    "    data_real_neighbourhood_count[0]\n",
    "    for data_real_neighbourhood_count in list_data_real_neighbourhood_count\n",
    "]\n",
    "counts = [\n",
    "    data_real_neighbourhood_count[1]\n",
    "    for data_real_neighbourhood_count in list_data_real_neighbourhood_count\n",
    "]\n",
    "\n",
    "# print(counts)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(data_real_neighbourhoods, counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_listing = df_listings_data_cleaned.shape[0]\n",
    "\n",
    "# print(total_number_listing)\n",
    "\n",
    "for neighbourhood, count in list_data_real_neighbourhood_count:\n",
    "    print(\"{0:<35} {1:>8}\".format(neighbourhood, count / total_number_listing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's detect the language of **neighborhood_overview**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob after somem request gives: HTTP Error 429: Too Many Requests\n",
    "\n",
    "# text_blob = TextBlob('la casa è brutta')\n",
    "\n",
    "# print(text_blob.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_language(text):\n",
    "#    try:\n",
    "#        return TextBlob(text).detect_language()\n",
    "#    except:\n",
    "#        return 'not detected'\n",
    "#\n",
    "# print(detect_language(22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching online I have found this very intresting [blog post](http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/) about language detection through counting the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\n",
    "\n",
    "\n",
    "def calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "\n",
    "    @param word_tokens: Tokenized text whose language want to be detected\n",
    "    @type text: str\n",
    "\n",
    "    @return: Dictionary with languages and unique stopwords seen in analyzed text\n",
    "    @rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "    words = [word.lower() for word in word_tokens]\n",
    "\n",
    "    # Compute per language included in nltk number of unique stopwords appearing in analyzed text\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements)  # language \"score\"\n",
    "\n",
    "    return languages_ratios\n",
    "\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return the highest scored.\n",
    "\n",
    "    It uses a stopwords based approach, counting how many unique stopwords\n",
    "    are seen in analyzed text.\n",
    "\n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "\n",
    "    @return: Most scored language guessed\n",
    "    @rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ratios = calculate_languages_ratios(text)\n",
    "        most_rated_language = max(ratios, key=ratios.get)\n",
    "    except Exception:\n",
    "        most_rated_language = \"not detected\"\n",
    "\n",
    "    return most_rated_language\n",
    "\n",
    "\n",
    "# input_text = \"This is a sample sentence, showing off the language detection\"\n",
    "input_text = \"Questa è una frase in italiano\"\n",
    "\n",
    "print(detect_language(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each neighborhood_overview with the detected language\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_listings_data_cleaned[\"detected_language\"] = [\n",
    "    detect_language(neighborhood_overview)\n",
    "    for neighborhood_overview in df_listings_data_cleaned[\"neighborhood_overview\"]\n",
    "]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_detected_language = list(df_listings_data_cleaned[\"detected_language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_detected_language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an histogram of the neighborhood_overview detected languages\n",
    "\n",
    "list_language_count = []\n",
    "\n",
    "for language in set(list_detected_language):\n",
    "    list_language_count.append((language, list_detected_language.count(language)))\n",
    "\n",
    "# print(list_language_count)\n",
    "\n",
    "languages = [language_count[0] for language_count in list_language_count]\n",
    "counts = [language_count[1] for language_count in list_language_count]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(languages, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_review = df_listings_data_cleaned.shape[0]\n",
    "\n",
    "# print(total_number_review)\n",
    "\n",
    "for language, count in list_language_count:\n",
    "    print(\"{0:<15} {1:>8}\".format(language, count / total_number_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same analysis must be done on the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned = df_reviews_data[\n",
    "    [\n",
    "        \"listing_id\",\n",
    "        # , 'date'\n",
    "        \"comments\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_reviews_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the missing values\n",
    "\n",
    "# Not very usefull because there is no missing value\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df_reviews_data_cleaned.isnull(), cmap=\"Blues\", cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df_reviews_data_cleaned with listings dataframe to link listing_id to the neighborhood\n",
    "\n",
    "df_listings_reviews = df_reviews_data_cleaned.join(\n",
    "    df_listings_data_cleaned[[\"id\", \"real_neighbourhood\"]].set_index(\"id\"),\n",
    "    on=\"listing_id\",\n",
    ")\n",
    "\n",
    "df_listings_reviews.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_review_data_real_neighbourhood = list(df_listings_reviews[\"real_neighbourhood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(list_review_data_real_neighbourhood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an histogram of the number of review related to a neighborhood\n",
    "\n",
    "list_review_data_real_neighbourhood_count = []\n",
    "\n",
    "for data_review_real_neighbourhood in set(list_review_data_real_neighbourhood):\n",
    "    list_review_data_real_neighbourhood_count.append(\n",
    "        (\n",
    "            str(data_review_real_neighbourhood),\n",
    "            list_review_data_real_neighbourhood.count(\n",
    "                str(data_review_real_neighbourhood)\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# print(list_review_data_real_neighbourhood_count)\n",
    "\n",
    "review_data_real_neighbourhoods = [\n",
    "    review_data_real_neighbourhood_count[0]\n",
    "    for review_data_real_neighbourhood_count in list_review_data_real_neighbourhood_count\n",
    "]\n",
    "counts = [\n",
    "    review_data_real_neighbourhood_count[1]\n",
    "    for review_data_real_neighbourhood_count in list_review_data_real_neighbourhood_count\n",
    "]\n",
    "\n",
    "# print(sum(counts))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(review_data_real_neighbourhoods, counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_listing = df_listings_data_cleaned.shape[0]\n",
    "\n",
    "# print(total_number_listing)\n",
    "\n",
    "for neighbourhood, count in list_review_data_real_neighbourhood_count:\n",
    "    print(\"{0:<35} {1:>8}\".format(neighbourhood, count / total_number_listing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each review with the detected language\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_reviews_data_cleaned[\"detected_language\"] = [\n",
    "    detect_language(comment) for comment in df_reviews_data_cleaned[\"comments\"]\n",
    "]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_detected_language = list(df_reviews_data_cleaned[\"detected_language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_detected_language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an histogram of the comments detected languages\n",
    "\n",
    "list_language_count = []\n",
    "\n",
    "for language in set(list_detected_language):\n",
    "    list_language_count.append((language, list_detected_language.count(language)))\n",
    "\n",
    "# print(list_language_count)\n",
    "\n",
    "languages = [language_count[0] for language_count in list_language_count]\n",
    "counts = [language_count[1] for language_count in list_language_count]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(languages, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_review = df_reviews_data_cleaned.shape[0]\n",
    "\n",
    "# print(total_number_review)\n",
    "\n",
    "for language, count in list_language_count:\n",
    "    print(\"{0:<15} {1:>8}\".format(language, count / total_number_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe\n",
    "\n",
    "df_listings_data_cleaned.to_csv(DATA_FOLDER + \"output_\" + \"listings.csv\")\n",
    "df_reviews_data_cleaned.to_csv(DATA_FOLDER + \"output_\" + \"reviews.csv\")\n",
    "\n",
    "# df_listings_data_cleaned = pd.read_csv(DATA_FOLDER + 'output_' + 'listings.csv')\n",
    "# df_reviews_data_cleaned = pd.read_csv(DATA_FOLDER + 'output_' + 'reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first step for sake of simplicity let's focus on neighborhood overview and reviews in English.\n",
    "Anyway some possible strategies to tackle the different languages could be:\n",
    "- Translate everything to English\n",
    "- Try to redo the same steps for other languages (first of Italian because is the second one more used)\n",
    "\n",
    "The words that gives us context are the one related to neighborhood:\n",
    "\n",
    "Synonyms neighborhood (Quartiere in Italian):\n",
    "- [`English`](https://www.thesaurus.com/browse/neighborhood):\n",
    " - area\n",
    " - block\n",
    " - district\n",
    " - ghetto\n",
    " - parish\n",
    " - part\n",
    " - precinct\n",
    " - region\n",
    " - section\n",
    " - slum\n",
    " - street\n",
    " - suburb\n",
    " - territory\n",
    " - zone\n",
    "- [`Italian`](https://dizionari.corriere.it/dizionario_sinonimi_contrari/Q/quartiere.shtml):\n",
    " - zona\n",
    " - vicinato\n",
    " - rione\n",
    " - sobborgo\n",
    " - borgata\n",
    " \n",
    " As already said let's focus on English and extract only the records marked like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned_eng = df_listings_data_cleaned[\n",
    "    df_listings_data_cleaned[\"detected_language\"] == \"english\"\n",
    "]\n",
    "\n",
    "# df_listings_data_cleaned_eng.set_index('id', inplace = True)\n",
    "\n",
    "df_listings_data_cleaned_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_listings_data_cleaned_eng.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng = df_reviews_data_cleaned[\n",
    "    df_reviews_data_cleaned[\"detected_language\"] == \"english\"\n",
    "]\n",
    "\n",
    "df_reviews_data_cleaned_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reviews_data_cleaned_eng.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In listing using **neighborhood_overview** we can directly get the sentiment of the neighborhood but for the reviews for the **comments** we have to extract the sentences only related to neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob = TextBlob(\"in my opinion textblob is very usefull\")\n",
    "\n",
    "# print(text_blob.detect_language()) # after some requests gives HTTP Error 429: Too Many Requests\n",
    "print(text_blob.tags)\n",
    "print(text_blob.words)\n",
    "print(text_blob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "print(get_polarity_sentiment(\"pippo is awesome\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each review with the neighborhood sentiment\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_listings_data_cleaned_eng[\"neighborhood_sentiment\"] = [\n",
    "    get_polarity_sentiment(row[\"neighborhood_overview\"])\n",
    "    for index, row in df_listings_data_cleaned_eng.iterrows()\n",
    "]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_data_cleaned_eng.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_neighbourhood_sentiment = df_listings_data_cleaned_eng[\n",
    "    [\"real_neighbourhood\", \"neighborhood_sentiment\"]\n",
    "]\n",
    "\n",
    "df_listings_neighbourhood_sentiment = df_listings_neighbourhood_sentiment.groupby(\n",
    "    [\"real_neighbourhood\"], as_index=False\n",
    ")[\"neighborhood_sentiment\"].mean()\n",
    "\n",
    "df_listings_neighbourhood_sentiment = df_listings_neighbourhood_sentiment.sort_values(\n",
    "    by=\"neighborhood_sentiment\", ascending=False\n",
    ")\n",
    "\n",
    "df_listings_neighbourhood_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's peek at the neighborhood_overview of the best and the worst scoring neighborhood\n",
    "\n",
    "list_neighborhood_overview = list(\n",
    "    df_listings_data_cleaned_eng[\n",
    "        df_listings_data_cleaned_eng[\"real_neighbourhood\"] == \"sempione\"\n",
    "    ][\"neighborhood_overview\"]\n",
    ")\n",
    "string_list_neighborhood_overview = \" \".join(list_neighborhood_overview)\n",
    "# print(string_list_neighborhood_overview)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "word_tokens = tokenizer.tokenize(string_list_neighborhood_overview)\n",
    "words = [word.lower() for word in word_tokens]\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# print(stop_words)\n",
    "\n",
    "filtered_list_neighborhood_overview = [w for w in words if w not in stop_words]\n",
    "filtered_string_list_neighborhood_overview = \" \".join(\n",
    "    filtered_list_neighborhood_overview\n",
    ")\n",
    "# print(filtered_string_list_neighborhood_overview)\n",
    "\n",
    "text_blob = TextBlob(filtered_string_list_neighborhood_overview)\n",
    "df_neighborhood_overview_word_count = pd.DataFrame(\n",
    "    (text_blob.word_counts).items(), columns=[\"word\", \"count\"]\n",
    ")\n",
    "df_neighborhood_overview_word_count = df_neighborhood_overview_word_count.sort_values(\n",
    "    by=\"count\", ascending=False\n",
    ")\n",
    "df_neighborhood_overview_word_count.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neighborhood_overview = list(\n",
    "    df_listings_data_cleaned_eng[\n",
    "        df_listings_data_cleaned_eng[\"real_neighbourhood\"] == \"figino\"\n",
    "    ][\"neighborhood_overview\"]\n",
    ")\n",
    "string_list_neighborhood_overview = \" \".join(list_neighborhood_overview)\n",
    "# print(string_list_neighborhood_overview)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "word_tokens = tokenizer.tokenize(string_list_neighborhood_overview)\n",
    "words = [word.lower() for word in word_tokens]\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# print(stop_words)\n",
    "\n",
    "filtered_list_neighborhood_overview = [w for w in words if w not in stop_words]\n",
    "filtered_string_list_neighborhood_overview = \" \".join(\n",
    "    filtered_list_neighborhood_overview\n",
    ")\n",
    "# print(filtered_string_list_neighborhood_overview)\n",
    "\n",
    "text_blob = TextBlob(filtered_string_list_neighborhood_overview)\n",
    "df_neighborhood_overview_word_count = pd.DataFrame(\n",
    "    (text_blob.word_counts).items(), columns=[\"word\", \"count\"]\n",
    ")\n",
    "df_neighborhood_overview_word_count = df_neighborhood_overview_word_count.sort_values(\n",
    "    by=\"count\", ascending=False\n",
    ")\n",
    "df_neighborhood_overview_word_count.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intresting to note that the sentiment is of course highly affected by how many words are present in the concatenation of all **neighborhood_overview**. For example for the worst 'figino' there is only one **neighborhood_overview** so the sentiment is really low compared to the others even if the actual **neighborhood_overview** is pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_neighbourhood_sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the sentiment of the neighbourhoods\n",
    "\n",
    "df_listings_neighbourhood_sentiment.hist(column=\"neighborhood_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the neighborhoods sentiment is skewed right because of course the hosts will in general give a more positive overview of the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an histogram of the detected sentiment of the neighborhoods\n",
    "\n",
    "neighborhoods = []\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_listings_neighbourhood_sentiment.iterrows():\n",
    "    neighborhoods.append(row[\"real_neighbourhood\"])\n",
    "    sentiments.append(row[\"neighborhood_sentiment\"])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(neighborhoods, sentiments)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's search in the review for the keywords related to neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_words_english = [\n",
    "    \"neighborhood\",\n",
    "    \"area\",\n",
    "    \"block\",\n",
    "    \"district\",\n",
    "    \"ghetto\",\n",
    "    \"parish\",\n",
    "    # , 'part' # removed beacuse was beeing used to mark not neighborhood related part of comments\n",
    "    \"precinct\",\n",
    "    \"region\",\n",
    "    \"section\",\n",
    "    \"slum\",\n",
    "    \"street\",\n",
    "    \"suburb\",\n",
    "    \"territory\",\n",
    "    \"zone\",\n",
    "    \"location\",\n",
    "]\n",
    "\n",
    "# searched_words_italian = ['quartiere'\n",
    "#                          , 'zona'\n",
    "#                          , 'vicinato'\n",
    "#                          , 'rione'\n",
    "#                          , 'sobborgo'\n",
    "#                          , 'borgata'\n",
    "#                          ]\n",
    "\n",
    "# sarched_words  = searched_words_english + searched_words_italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_words(text, searched_words):\n",
    "    try:\n",
    "        for word in searched_words:\n",
    "            if word in text:\n",
    "                return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "print(detect_words(\"questo testo continene pippo\", [\"pippo\", \"pluto\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize_text(text, language):\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "        word_tokens = tokenizer.tokenize(text)\n",
    "        stop_words = set(stopwords.words(language))\n",
    "        filtered_sentence = [word for word in word_tokens if word not in stop_words]\n",
    "        return filtered_sentence\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "input_text = \"This is a sample sentence, showing off the stop words filtration!!!\"\n",
    "print(clean_tokenize_text(input_text, \"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and clean comments\n",
    "comment, language = df_reviews_data_cleaned_eng[[\"comments\", \"detected_language\"]].iloc[\n",
    "    0\n",
    "]\n",
    "\n",
    "print(comment, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_tokenize_text(comment, language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detect_words(clean_tokenize_text(comment, language), searched_words_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each review if searched words are present\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_reviews_data_cleaned_eng[\"contains_searched_words\"] = [\n",
    "    detect_words(\n",
    "        clean_tokenize_text(row[\"comments\"], row[\"detected_language\"]),\n",
    "        searched_words_english,\n",
    "    )\n",
    "    for index, row in df_reviews_data_cleaned_eng.iterrows()\n",
    "]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_contains_serached_words = list(\n",
    "    df_reviews_data_cleaned_eng[\"contains_searched_words\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_contains_serached_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(list_contains_serached_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(list_contains_serached_words) / df_reviews_data_cleaned_eng.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 41% of the english review contains some words related to the neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider only records containing the searched words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng_contains = df_reviews_data_cleaned_eng[\n",
    "    df_reviews_data_cleaned_eng[\"contains_searched_words\"] is True\n",
    "]\n",
    "\n",
    "df_reviews_data_cleaned_eng_contains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the goal is to isolate the words related to neighborhood or similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = df_reviews_data_cleaned_eng_contains[\"comments\"].iloc[0]\n",
    "\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at some comments I had realize I cuold use punctuation to isolate the phares related to neighborhood instad of remove it like I ws rhinking at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextual_phrase(text, language, searched_words):\n",
    "    contextual_phrase = \"\"\n",
    "    sentences = text.split(\".\")\n",
    "    for sentence in sentences:\n",
    "        if (\n",
    "            detect_words(clean_tokenize_text(sentence, language), searched_words)\n",
    "            is True\n",
    "        ):\n",
    "            contextual_phrase = contextual_phrase + \" \" + sentence\n",
    "    if contextual_phrase == \"\":\n",
    "        return text\n",
    "    else:\n",
    "        return contextual_phrase\n",
    "\n",
    "\n",
    "text = \"Staying at Francesca's and Alberto's place was a pleasure. Just as described, well located for my purposes, an enjoyable walk to the Tortona area. The room is very nice, cleaned daily and has private bathroom.Francesca is super friendly and very helpful; whilst still respecting privacy. Overall a great experience!\"\n",
    "\n",
    "print(get_contextual_phrase(text, \"english\", searched_words_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each review with contextual phrases\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_reviews_data_cleaned_eng_contains[\"contextual_phrases\"] = [\n",
    "    get_contextual_phrase(\n",
    "        row[\"comments\"], row[\"detected_language\"], searched_words_english\n",
    "    )\n",
    "    for index, row in df_reviews_data_cleaned_eng_contains.iterrows()\n",
    "]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng_contains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the sentiment of the neighborhood retated sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = df_reviews_data_cleaned_eng_contains[\"contextual_phrases\"].iloc[0]\n",
    "\n",
    "print(comment)\n",
    "\n",
    "text_blob = TextBlob(comment)\n",
    "\n",
    "print(text_blob.tags)\n",
    "print(text_blob.words)\n",
    "print(text_blob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each review with the neighborhood sentiment\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_reviews_data_cleaned_eng_contains[\"neighborhood_sentiment\"] = [\n",
    "    get_polarity_sentiment(row[\"contextual_phrases\"])\n",
    "    for index, row in df_reviews_data_cleaned_eng_contains.iterrows()\n",
    "]\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_data_cleaned_eng_contains.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_listing_neighbourhood_sentiment = df_reviews_data_cleaned_eng_contains[\n",
    "    [\"listing_id\", \"neighborhood_sentiment\"]\n",
    "]\n",
    "\n",
    "df_reviews_listing_neighbourhood_sentiment = (\n",
    "    df_reviews_listing_neighbourhood_sentiment.groupby([\"listing_id\"], as_index=False)[\n",
    "        \"neighborhood_sentiment\"\n",
    "    ].mean()\n",
    ")\n",
    "\n",
    "df_reviews_listing_neighbourhood_sentiment = (\n",
    "    df_reviews_listing_neighbourhood_sentiment.sort_values(\n",
    "        by=\"neighborhood_sentiment\", ascending=False\n",
    "    )\n",
    ")\n",
    "\n",
    "df_reviews_listing_neighbourhood_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_listing_neighbourhood_sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df_reviews_neighbourhood_sentiment with original dataframe to link sentiment to the neighborhood\n",
    "\n",
    "df_listings_reviews_sentiment = df_listings_data_cleaned.join(\n",
    "    df_reviews_listing_neighbourhood_sentiment.set_index(\"listing_id\"), on=\"id\"\n",
    ")[[\"id\", \"real_neighbourhood\", \"neighborhood_sentiment\"]]\n",
    "\n",
    "df_listings_reviews_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's peek at some records\n",
    "\n",
    "print(df_reviews_data_cleaned[\"comments\"].iloc[6400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_neighbourhood_sentiment = df_listings_reviews_sentiment[\n",
    "    [\"real_neighbourhood\", \"neighborhood_sentiment\"]\n",
    "]\n",
    "\n",
    "df_reviews_neighbourhood_sentiment = df_reviews_neighbourhood_sentiment.groupby(\n",
    "    [\"real_neighbourhood\"], as_index=False\n",
    ")[\"neighborhood_sentiment\"].mean()\n",
    "\n",
    "df_reviews_neighbourhood_sentiment = df_reviews_neighbourhood_sentiment.sort_values(\n",
    "    by=\"neighborhood_sentiment\", ascending=False\n",
    ")\n",
    "\n",
    "df_reviews_neighbourhood_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_reviews_neighbourhood_sentiment[\"real_neighbourhood\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_neighbourhood_sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the sentiment of the neighbourhoods\n",
    "\n",
    "df_reviews_neighbourhood_sentiment.hist(column=\"neighborhood_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the neighborhoods sentiment is more gaussian distributed with respet to the one given by the **neighbourhood_overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an histogram of the detected sentiment of the neighborhoods\n",
    "\n",
    "neighborhoods = []\n",
    "sentiments = []\n",
    "\n",
    "for index, row in df_reviews_neighbourhood_sentiment.iterrows():\n",
    "    neighborhoods.append(row[\"real_neighbourhood\"])\n",
    "    sentiments.append(row[\"neighborhood_sentiment\"])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(neighborhoods, sentiments)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results of a sentiment analysis on the **neighborhood_overview** column in the listing dataframe with the reviews one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_neighbourhood_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_neighbourhood_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment = pd.DataFrame(\n",
    "    list_real_neighbourhood, columns=[\"real_neighbourhood\"]\n",
    ")\n",
    "\n",
    "df_neighbourhood_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment = df_neighbourhood_sentiment.set_index(\n",
    "    \"real_neighbourhood\"\n",
    ").join(\n",
    "    df_listings_neighbourhood_sentiment.set_index(\"real_neighbourhood\"),\n",
    "    rsuffix=\"_listing\",\n",
    ")\n",
    "df_neighbourhood_sentiment.rename(\n",
    "    columns={\"neighborhood_sentiment\": \"neighborhood_sentiment_listing\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment = df_neighbourhood_sentiment.join(\n",
    "    df_reviews_neighbourhood_sentiment.set_index(\"real_neighbourhood\"),\n",
    "    rsuffix=\"_review\",\n",
    ")\n",
    "df_neighbourhood_sentiment.rename(\n",
    "    columns={\"neighborhood_sentiment\": \"neighborhood_sentiment_review\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment = df_neighbourhood_sentiment.sort_values(\n",
    "    by=\"neighborhood_sentiment_review\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment_dropna = df_neighbourhood_sentiment.dropna(\n",
    "    subset=[\"neighborhood_sentiment_review\"], axis=0\n",
    ")\n",
    "df_neighbourhood_sentiment_dropna.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment[\"difference\"] = [\n",
    "    row[\"neighborhood_sentiment_listing\"] - row[\"neighborhood_sentiment_review\"]\n",
    "    for index, row in df_neighbourhood_sentiment.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment[df_neighbourhood_sentiment[\"difference\"] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbourhood_sentiment[df_neighbourhood_sentiment[\"difference\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty intresting to note that the majority of **neighborhood_sentiment_review** are bigger than **neighborhood_sentiment_listing**. One possible explanation could be that in the field **neighborhood_overview** the host usually tend to use a lot of words to describe the neighborhood while extracting only the sentences related to neighborhood clean up more the string used for the sentiment analysis giving an overall higher score.\n",
    "Let's do a word cloud to compare the words used to get **neighborhood_sentiment_listing** and **neighborhood_sentiment_review**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_neighborhood = [\n",
    "    \"sempione\",\n",
    "    \"brera\",\n",
    "    \"duomo\",\n",
    "    \"ticinese\",\n",
    "    \"quarto oggiaro\",\n",
    "    \"bovisasca\",\n",
    "    \"comasina\",\n",
    "    \"quinto romano\",\n",
    "]\n",
    "\n",
    "for neighborhood in show_neighborhood:\n",
    "    list_neighborhood_overview = list(\n",
    "        df_listings_data_cleaned_eng[\n",
    "            df_listings_data_cleaned_eng[\"real_neighbourhood\"] == neighborhood\n",
    "        ][\"neighborhood_overview\"]\n",
    "    )\n",
    "    string_list_neighborhood_overview = \" \".join(list_neighborhood_overview)\n",
    "    # print(string_list_neighborhood_overview)\n",
    "\n",
    "    list_contextual_phrases = list(\n",
    "        (\n",
    "            df_listings_data_cleaned_eng[\n",
    "                df_listings_data_cleaned_eng[\"real_neighbourhood\"] == neighborhood\n",
    "            ]\n",
    "        ).join(\n",
    "            df_reviews_data_cleaned_eng_contains[\n",
    "                [\"listing_id\", \"contextual_phrases\"]\n",
    "            ].set_index(\"listing_id\"),\n",
    "            on=\"id\",\n",
    "        )[\"contextual_phrases\"]\n",
    "    )\n",
    "    # print(list_contextual_phrases)\n",
    "    list_contextual_phrases = [x for x in list_contextual_phrases if str(x) != \"nan\"]\n",
    "    string_list_contextual_phrases = \" \".join(list_contextual_phrases)\n",
    "    # print(string_list_contextual_phrases)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.update(searched_words_english)\n",
    "    stop_words.update([neighborhood, \"milan\", \"city\", \"apartment\"])\n",
    "    stop_words.remove(\"not\")\n",
    "    stop_words.remove(\"no\")\n",
    "    stop_words.remove(\"nor\")\n",
    "    # print(stop_words)\n",
    "\n",
    "    f = plt.figure(figsize=(20, 10))\n",
    "    f.suptitle(neighborhood)\n",
    "    ax = f.add_subplot(121)\n",
    "    ax2 = f.add_subplot(122)\n",
    "\n",
    "    ax.set_title(\"listing_neighborhood_overview\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    ax2.set_title(\"review_contextual_phrases\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    try:\n",
    "        wordcloud_neighborhood_overview = WordCloud(stopwords=stop_words).generate(\n",
    "            string_list_neighborhood_overview\n",
    "        )\n",
    "        ax.imshow(wordcloud_neighborhood_overview, interpolation=\"bilinear\")\n",
    "\n",
    "        wordcloud_contextual_phrases = WordCloud(stopwords=stop_words).generate(\n",
    "            string_list_contextual_phrases\n",
    "        )\n",
    "        ax2.imshow(wordcloud_contextual_phrases, interpolation=\"bilinear\")\n",
    "    except Exception:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "write-a-data-science-blog-post-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
